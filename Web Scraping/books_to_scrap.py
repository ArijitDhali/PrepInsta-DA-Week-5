# -*- coding: utf-8 -*-
"""Books_To_Scrap.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LZ8beV5ivag1DyIQusUH7mXhEm9PCv9W

#**Week-5 Assignment**
##**Web Scraping**
### *By Arijit Dhali [Linkedin](https://www.linkedin.com/in/arijit-dhali-b255b0138/)*

---

The aim for this asssignment is to scrap a website of book seller :
[Books to Scrap](http://books.toscrape.com/)<br>

From this website, we need to create a dataframe table with following columns:
* `title`
* `rating`
* `price`
* `link`

We need to use multiple libraries for this assignment which are: `requests` , `BeautifulSoup` and `pandas`

##**Import the Libraries**

First we will import the libraries required to scrap the data from the website.
<br>
The libraries required for the operation:
* `requestd` : For handling HTTP Request
* `BeautifulSoap` : Used for parsing and extracting the elements
* `csv` : To handle manipulate the .csv files
* `pandas` : For Data Manipulation
"""

import requests                           # For HTTP request handling
from bs4 import BeautifulSoup as bs       # Used for HTML parsing
import csv                                # In order to handle CSV file
import pandas as pd                       # Data manipulation

"""##**Get URL and send GET request**

In order to get the data from the rquired website:
* We will use the `requests` library to send request to the website
* If request is accepted, then show the status
"""

url = "http://books.toscrape.com/"
response = requests.get(url)        # Sending a request to the specified URL

if response.status_code == 200:     # Checking if the request was successful
    print("Request Successful")     # Printing a success message if the status code is 200
else:
    print("Request Failed")         # Printing a failure message if the status code is not 200

"""##**Parse the HTML Content**

After successfully getting the data, we will first view the format of `HTML` text, till 1000 characters.
"""

print(response.text[:1000])         # Printing the first 1000 characters of the response text

"""After viewing, we will sparse the `HTML` file using `BeautifulSoup` library."""

soup = bs(response.text, "html.parser")     # Creating a BeautifulSoup object for HTML parsing
print(type(soup))                           # Printing the type of the 'soup' object

"""##**Extract Details for 1 Book**

In order to get successful result, we will follow the following steps:
1. *Scrap the data of 1 Book*
2. *Scrap the data of all the books in 1 page*
3. *Scrap the data pf all the books of all 50 pages*

First we will find all the `<article>` tags in the website.<br>
Then we will print and view the first content of `<article>` tag.
"""

books = soup.find_all('article', class_='product_pod')    # Finding all HTML elements with the specified class
single_book = books[0]                                    # Accessing the first book element
single_book                                               # Printing the details of the first book element

"""Now we will extract the `title` attribute value from the first book element of `<anchor>` tag."""

title = single_book.find('a', title=True)['title']  # Extracting the 'title' attribute value from the first book element
title

"""Now we will extract the `star-rating` class value from the first book element of `<paragraph>` tag."""

rating = single_book.find('p', class_='star-rating')['class'][1]  # Extracting the rating class value from the first book element
rating

"""Now we will extract and clean the `price_color` class value from the first book element of `<paragraph>` tag."""

price = single_book.find('p', class_='price_color').text.strip().strip('Â')  # Extracting and cleaning the price of the first book
price

"""Now we will extract the `href` attribute value from the first book element of `<anchor>` tag.<br>
After that, we will concatenate the initial `url` to `book_url`.
"""

book_url = single_book.find('a')['href']        # Extracting the URL for the first book
link = url + book_url                           # Creating the complete URL for the book
link

"""##**Extract Book Details for 1 Page**

Using BeautifulSoup to find and extract book details from a single webpage:

1. Finds all `HTML` elements representing individual books.
2. Initializes an empty list to store book details.
3. For each book:
    - Extracting the book's title.
    - Extracting the book's rating.
    - Cleaning and extracting the book's price.
    - Extracting the book's URL and creating a complete link.
    - Appending all these details to a list.
"""

books = soup.find_all('article', class_='product_pod')  # Finding all book elements
books_data = []                                         # List to store book details

for book in books:                                                            # Iterating through each book element
    title = book.find('a', title=True)['title']                               # Extracting the title of the book
    rating = book.find('p', class_='star-rating')['class'][1]                 # Extracting the rating of the book
    price = book.find('p', class_='price_color').text.strip().strip('Â')      # Extracting and cleaning the price
    book_url = book.find('a')['href']                                         # Extracting the URL for the book
    link = url + book_url                                                     # Creating the complete URL for the book

    books_data.append([title, rating, price, link])                           # Appending book details to the list

"""Creating a DataFrame using `Pandas`.

"""

page = pd.DataFrame(books_data, columns=["title","rating","price","link"])    # Creating a DataFrame from books_data
page

"""##**Extract Book Details for All 50 Pages**

Firstly, using a `for` loop to iterate over page numbers from `1 to 50 (inclusive)`. <br>Then constructing the `URL` for each page, using f-string formatting. <br>After that we will print and display each generated page `URL` during the loop execution.
"""

for page_num in range(1, 51):                                               # Looping through pages from 1 to 50
    page_url = f'http://books.toscrape.com/catalogue/page-{page_num}.html'  # Generating the URL for each page
    print(page_url)                                                         # Printing and viewing the generated page URL

"""For collects book details from a website consisting of 50 Webpages, we will use two links:
* `primary_url`: A starting link used to build complete book URLs.
* `page_url`: A link to specify the directory of multiple webpages.

To extract data from 50 webpages, we will:
* First iterate through page numbers from 1 to 50.
* Construct the URL for each page on the website.
* Send a request to the page URL to get its content.
* Parsing the HTML content using BeautifulSoup.
* Find all elements representing individual books on the page.
* For each book, extracts its title, rating, price, and URL.
* Constructs the complete book URL by combining the primary URL with the book's specific URL.
* Gathers all these details into a list called `books_50_data`.

"""

primary_url = "http://books.toscrape.com/"                                  # Link to concatenate later
books_50_data = []                                                          # List to store book details from multiple pages

for page_num in range(1, 51):                                               # Looping through page numbers from 1 to 50
    page_url = f'http://books.toscrape.com/catalogue/page-{page_num}.html'  # Generating the URL for each page
    response = requests.get(page_url)                                       # Sending a request to the page URL
    soup_page = bs(response.text, "html.parser")                            # Creating a BeautifulSoup object for HTML parsing
    books = soup_page.find_all('article', class_='product_pod')             # Finding all book elements on the page

    for book in books:                                                      # Iterating through each book element
        title = book.find('a', title=True)['title']                         # Extracting the title of the book
        rating = book.find('p', class_='star-rating')['class'][1]           # Extracting the rating of the book
        price = book.find('p', class_='price_color').text.strip().strip('Â')  # Extracting and cleaning the price
        book_url = book.find('a')['href']                                   # Extracting the URL for the book
        link = primary_url + book_url                                       # Creating the complete URL for the book

        books_50_data.append([title, rating, price, link])                  # Appending book details to the list

"""Creating a DataFrame using `Pandas`.

"""

page_50 = pd.DataFrame(books_50_data, columns=["title","rating","price","link"])    # Creating a DataFrame from books_50_data
page_50

"""##**Export the Data**

Saving the final data to `books_scraped.csv` file in local machine.
"""

page_50.to_csv("books_scraped.csv", index = False)  # Saving the DataFrame to a CSV file without including the index
print("Data saved to .csv")                         # Printing a message confirming the data has been saved